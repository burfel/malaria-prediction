# Discussion

## Technical part
<big>
Creating a good model is difficult due to specific characteristics of the data (eg outliers), especially in the case where few data are available.
In our case a parametric model was appropriate as we could identify linear relationships between variables and the response variable and the distribution of the residuals in our linear model (with parasitemia percentage as independent variable) showed fairly normal distributions (see chapter _Methods_). Ridge regression to accommodate for multicollinearity was not relevant since we aimed for a simple model with few independent variables; instead we used variable selection. For more elaborate Machine Learning (ML) methods, including Random Forest (RF) and Support Vector Machines (SVMs), there was not enough data available.
We wanted to avoid such models that generate highly complex, non-linear relationships between the predictors and the outcome. Also, we wanted to keep the model interpretable; more complex models quickly tend to lose interpretability and more importantly, overfit the data.
Another aspect we considered when designing the models was the type of data a user is most likely to have. 
Our model(s) still need to be properly validated on independent data. Although we could achieve significant p-value for the F-test (< 0.05) for both the simple and the complex model (see chapter _Results_), we did not achieve a high R-squared value (< 0.4) that indicates how much variation in the output variable is explained by the input variables. Most of the variation (1-R-squared) is still unaccounted for. This might be due to measurement errors, the majority is probably noise. 
Once more data has become available, it might be meaningful to explore more elaborate non-parametric predictive models that are more powerful. 
We might want to consider models where outliers do not have much influence on the model, eg tree-based classification models if we formulate our problem as a classification problem or lasso and elastic-net regularized linear models. Also SVMs for classification generally disregard a part of the training set (that may be far away from the decision boundary and outside of the data mainstream) when creating a prediction equation. However, there is always a trade-off between performance on existent data set and over-fitting, and also the loss of interpretability in case of ML methods. In any case, the complex model should be properly validated.

However, this was not the purpose of this project.

## Practical relevance

### What does the tool allow people to do?

### Who might use the tool?

### Limitations
The model generated was built based on one single data set in Gambian children (see chapter _Data_ [INSERT LINK HERE]). It might very likely be not generalisable to other settings and ages. Also, it constists only of one type of RNA-seq data (Ribosomal RNA depleted, Globin Depleted RNA), and may not be directly equivalent to results form other preparation methods, eg PolyA [INSERT REFERENCE HERE].

<!--
Thre predictive performance, provided the complex model is properly validated, may compensate the loss in interpretation. 
-->

## Outlook

For more complex models, one might want to incorporate the information of the developmental stage of the disease (given by the surrogate proportion variables (SPVs) at different time points in the life cycle) as well the level of gametocytes (Gam5) representing the sexual stage of the parasite.
The observations found in chapter _Data_ [INSERT LINK HERE] might help for that.

Also, additional insight might be gained by comparing different species. We developed a logistic regression on the mouse data set; however, comparison to our models behind this web tool here, turned out to be difficult as it was not based on the same measurements and very few data was available. 

Once more data has become available, we might also want analyse with respect to different subject ID groups.
</big>
<!---
Main problems

- not enough data

- Machine Learning methods not applied since overfitting and interpretability...

- outliers, dimension of data
transformations for multiple predictors

-->

<!--
If the model is very senstive to outliers (which is not the case here), we could use the __spaial sign__ transformation (Seernels et al. 2006 INSERT IN REFERENCE LIST) which projects the predictor values onto a multidimensional sphere and thus makes all the samples the same distance from the centre of the sphere. This is mathematically done by each sample divided by its squared norm INSERT FORMULA HERE. One should note that centering and scaling of the predictor data is necessary because the denominator is intended to measure the squared distance to the centre of thre predictor's distribution. Also, this method transforms the predictors as a group, so we should not remove predictor variables after it. 
-->
