## Methods: Cross-validation / estimating performance of model(s)
<big>
Comparing the peformance of the linear model (with and without variable transformation) and [beta regression](http://r-statistics.co/Beta-Regression-With-R.html) proved difficult. There was very limited data available: We had 6 samples that were not used for the model based on the 40 samples that were complete on the selcted model variables; and 25 samples for the model that was based on the 21 fully complete samples (see chapter _Results_ for the graph of the regression lines). No independent test data set was available. 

For training the model we did not use the samples that contained missing values but instead, we used an imputed version of them (using the [Predictive Mean Matching (PMM) method](http://journals.sagepub.com/doi/abs/10.1177/0049124197026001001)) that could serve as a test data set.

Also because of the limited size of data we had, we performed 5-fold crossvalidation on the original training data to assess the predictive power of the model; crossvalidation can handle missing values, too. When we repeated crossvalidation multiple times (10 times), we received R-squared values between 0.35 abd 0.5 for percentage of parasitemia and 0.44 and 0.55 for parasitemia density. For the complex model (with >1 independent variables) we reported values between 0.45 and 0.55 for the one including parsitemia density; for the one with percentage of parasitemia we got similar values around R-squared values of 0.4. However, the predictions for each fold varied a lot; even for the big (42 samples) model. It seems like there is not much difference in predictive power between the simple and the complex model.

However, we should be careful regarding the choice of K in the fold as this can highly bias the results we get. Also it might me questionable to use this method on our small models where we performed variable selection mainly based on correlations and, therefore, created an "unfair advantage"" in favour of the model. 
</big>
